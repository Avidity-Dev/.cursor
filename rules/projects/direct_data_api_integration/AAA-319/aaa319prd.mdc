---
description: 
globs: 
alwaysApply: true
---
# AAA-319: Veeva CRM DLT Multi-File Asset (ADLS to Snowflake)

## Table of Contents

- [1. Motivation & Context](mdc:#1-motivation--context)
- [2. Project Goals](mdc:#2-project-goals)
- [3. Scope](mdc:#3-scope)
  - [3.1. In Scope](mdc:#31-in-scope)
  - [3.2. Out of Scope](mdc:#32-out-of-scope)
- [4. Technical Approach](mdc:#4-technical-approach)
  - [4.1. Overview](mdc:#41-overview)
  - [4.2. Key Components](mdc:#42-key-components)
    - [4.2.1. Multi-File Asset Definition](mdc:#421-multi-file-asset-definition)
    - [4.2.2. ADLS Configuration](mdc:#422-adls-configuration)
    - [4.2.3. Snowflake Schema & Table Naming](mdc:#423-snowflake-schema--table-naming)
    - [4.2.4. Pipeline Execution](mdc:#424-pipeline-execution)
  - [4.3. Configuration Parameters](mdc:#43-configuration-parameters)
  - [4.4. Integration with Existing Assets](mdc:#44-integration-with-existing-assets)
- [5. Implementation Tasks](mdc:#5-implementation-tasks)
- [6. Success Criteria](mdc:#6-success-criteria)
- [7. Key Dependencies & Assumptions](mdc:#7-key-dependencies--assumptions)
- [8. Future Considerations (Post-MVP)](mdc:#8-future-considerations)
- [9. Documentation Requirements](mdc:#9-documentation-requirements)

## 1. Motivation & Context

This document outlines the requirements for the linear issue AAA-319 of the Veeva CRM data integration project (Linear Project: Direct Data API Integration), focusing on loading the unpacked CSV files currently stored in ADLS (done in AAA-318) into Snowflake using DLT. This is the final step in the ELT process for Veeva CRM data retrieved via the Direct Data API.

The related Linear issues in this workflow are:
1. AAA-317: Retrieved raw data archives from Veeva CRM via Direct Data API and stored in ADLS.
    - [aaa317prd.md](mdc:.dev/projects/PRJ_018_DirectDataAPI/MVP/AAA-317/aaa317prd.md)
    - [aaa317report.md](mdc:.dev/projects/PRJ_018_DirectDataAPI/MVP/AAA-317/aaa317report.md)
2. AAA-318: Unpacked the raw archives into individual CSV files and stored them in a structured location in ADLS.
    - [aaa318prd.md](mdc:.dev/projects/PRJ_018_DirectDataAPI/MVP/AAA-318/aaa318prd.md)
    - [aaa318report.md](mdc:.dev/projects/PRJ_018_DirectDataAPI/MVP/AAA-318/aaa318report.md)

This task (AAA-319) completes the data pipeline by loading these extracted CSV files into Snowflake, making the data available for analytics, dbt transformations, and business intelligence.

## 2. Project Goals

* Develop a Dagster asset that utilizes DLT's multi-file loading capabilities to read CSV files from ADLS and load them into Snowflake
* Configure the asset to source data from the ADLS location where unpacked Veeva CRM CSV files reside (output of AAA-318)
* Define appropriate schema and table naming conventions in Snowflake for the Veeva CRM data
* Implement proper error handling and logging for the loading process
* Complete the end-to-end data pipeline from Veeva CRM to Snowflake to support downstream data consumers

## 3. Scope

### 3.1. In Scope

* **DLT Multi-File Asset Definition**:
  * Create a new `@multi_file_assets` definition in `prometheus/dlt/sources/veeva/assets.py`
  * Configure the asset to use the `MultiFileResource` for processing CSV files from ADLS
  * Define base names that correspond to the expected Veeva CRM models/tables
  * Configure destination schema and table prefix for Snowflake

* **ADLS Integration**:
  * Configure the `bucket_url` parameter to point to the ADLS location where unpacked Veeva CRM CSV files reside
  * Ensure proper authentication for accessing ADLS

* **File Processing**:
  * Process CSV files found in the designated ADLS path
  * Handle file name mapping to table names if necessary

* **Snowflake Loading**:
  * Load the CSV data into Snowflake tables under the defined schema and with appropriate table prefixes (crm__)
  * Configure appropriate write disposition (replace/append)

* **Orchestration & Error Handling**:
  * Configure the asset to be triggerable either manually or based on the completion of AAA-318 asset (unpacked CSV files) called `unpacked_tarball`
  * Implement basic error handling for common failure scenarios

### 3.2. Out of Scope

* **Data Transformation/Validation**: Complex data transformation or validation of the loaded data (this will be handled by downstream dbt transformations)
* **Schema Evolution**: Automatic handling of schema changes in source data
* **Incremental Loading Logic**: Building a sophisticated incremental loading mechanism (beyond what is provided by DLT)
* **Historical Data Backfill**: Loading historical data from before the implementation
* **Alerting & Monitoring**: Setting up advanced alerting and monitoring for the pipeline beyond basic Dagster logging
* **User Interface**: Building a custom UI for monitoring or triggering the pipeline

## 4. Technical Approach

### 4.1. Overview

The solution will implement a new multi-file asset in the Dagster framework, utilizing the existing DLT integration components. This asset will be configured to:

1. Source CSV files from the "unpacked" ADLS location populated by the AAA-318 asset
2. Use the `MultiFileResource` to process these files
3. Define appropriate mapping between file names and Snowflake table names
4. Load the data into Snowflake under a specified schema with table prefixes
5. Output material results with metadata about the loaded tables

The implementation will follow the existing pattern established for other Veeva data sources (like the customer and opendata multi-assets), adapted for the CRM Direct Data API source.

### 4.2. Key Components

#### 4.2.1. Multi-File Asset Definition

A new `@multi_file_assets` definition will be created in `prometheus/dlt/sources/veeva/assets.py`. This will follow the pattern established by the existing `customer_multi_asset` and `opendata_multi_asset` functions.

```python
# Example of the new multi-file asset definition
@multi_file_assets(
    name="veeva_crm_direct_data_multi_asset",
    group_name="veeva_crm_direct_data_assets",
    base_names=VEEVA_CRM_DIRECT_DATA_MODELS,  # Defined in config.py
    dest_schema=VEEVA_CRM_DIRECT_DATA_DEST_SCHEMA,  # Defined in config.py
    table_prefix=VEEVA_CRM_DIRECT_DATA_ASSET_PREFIX,  # Defined in config.py
)
async def veeva_crm_direct_data_multi_asset(
    context: dg.AssetExecutionContext,
    veeva_file_loader: MultiFileResource,
    config: LoadFilesRunConfig,
):
    """
    Multi-asset for loading Veeva CRM Direct Data API files from ADLS to Snowflake.
    
    This asset sources unpacked CSV files from ADLS (output of the Phase 2 unpacking asset)
    and loads them into Snowflake tables.
    """
    # Parse run_config or use direct config
    run_config = context.run.run_config["ops"]["veeva_crm_direct_data_multi_asset"]["config"]
    
    try:
        # Validate the run config
        load_config = LoadFilesRunConfig(**run_config)
        context.log.info(
            f"[Veeva | CRM | Direct Data] Received run config: bucket_url='{load_config.bucket_url}'"
            f", files={load_config.file_names}"
        )
    except Exception as e:
        context.log.error(
            f"[Veeva | CRM | Direct Data] Failed to parse run_config: {run_config}. Error: {e}",
            exc_info=True,
        )
        raise Exception("Invalid run configuration.") from e
    
    if not load_config.file_names:
        context.log.warning(
            "[Veeva | CRM | Direct Data] No file names provided. Skipping."
        )
        return
    
    # Run the file loader to process files and load to Snowflake
    async for result in veeva_file_loader.run(
        context=context,
        load_config=load_config,
        bucket_url=load_config.bucket_url,
        # No credentials needed if using ADLS and authentication is handled via environment
    ):
        yield result
```

#### 4.2.2. ADLS Configuration

The asset will be configured to read from the ADLS location where the unpacked Veeva CRM CSV files are stored (output of AAA-318). This will use the `bucket_url` parameter of the `LoadFilesRunConfig` to specify the ADLS path.

The `bucket_url` will need to be formatted as an ADLS URL, which typically follows the pattern:
```
https://<storage_account>.blob.core.windows.net/<container>/<path>
```

For this implementation, the path will point to the location where the unpacked CSV files are stored, following the structure defined in AAA-318: 
```
[unpacked_base_prefix]/unpacked_[archive_name_without_ext]/
```

#### 4.2.3. Snowflake Schema & Table Naming

The destination schema and table prefix will be defined in the `prometheus/dlt/sources/veeva/config.py` file, following the pattern established for other Veeva data sources:

```python
# Example additions to config.py
VEEVA_CRM_DIRECT_DATA_MODELS = dlt.config["veeva.crm.direct_data.models"]
VEEVA_CRM_DIRECT_DATA_ASSET_PREFIX = dlt.config["veeva.crm.direct_data.asset_prefix"]
VEEVA_CRM_DIRECT_DATA_DEST_SCHEMA = dlt.config["veeva.crm.direct_data.dest_schema"]
```

These values will be sourced from the `.dlt/config.toml` file, which will need to be updated to include:

```toml
[veeva.crm.direct_data]
models = ["document", "object", "metadata", "picklist"]  # Example model names, to be updated based on actual CSV files
asset_prefix = "crm"
dest_schema = "ingest_veeva"
```

#### 4.2.4. Pipeline Execution

The DLT pipeline will be executed through the `MultiFileResource.run()` method, which handles the following steps:
1. Builds a DLT pipeline for each CSV file
2. Executes the pipelines in parallel
3. Loads the data into Snowflake tables
4. Returns materialization results for each processed file/table

### 4.3. Configuration Parameters

The following configuration parameters will be needed:

* **MultiFileResource Configuration**:
  * `destination`: Set to "snowflake" for Snowflake destination
  * `pipeline_opts`: Optional parameters for the DLT pipeline

* **LoadFilesRunConfig**:
  * `bucket_url`: ADLS URL where the unpacked CSV files are located
  * `file_names`: List of specific CSV file paths to process
  * `name_mapping`: Dictionary mapping file names to base names (model names)
  * `dataset_name`: Schema name in Snowflake (e.g., "ingest_veeva")
  * `table_prefix`: Table prefix for Snowflake tables (e.g., "crm")

### 4.4. Integration with Existing Assets

The new asset should be able to operate independently, given the appropriate configuration. It can be triggered manually or potentially configured to run upon successful completion of the AAA-318 asset.

The configuration should allow for flexibility in specifying which CSV files to process, enabling both batch processing of all files or selective processing of specific files.

## 5. Implementation Tasks

1. **Update Configuration**:
   * Add Veeva CRM Direct Data configuration to `.dlt/config.toml`
   * Add corresponding constants to `prometheus/dlt/sources/veeva/config.py`

2. **Create Multi-File Asset**:
   * Implement `veeva_crm_direct_data_multi_asset` in `prometheus/dlt/sources/veeva/assets.py`
   * Configure with appropriate parameters for ADLS source and Snowflake destination

3. **Update Dagster Definitions**:
   * Update any relevant sections in `prometheus/dlt/definitions.py` to include the new asset
   * Ensure necessary resources (e.g., `MultiFileResource`) are configured and available

4. **Testing**:
   * Local testing with sample CSV files
   * Integration testing with actual unpacked Veeva CRM files from ADLS to Snowflake

5. **Documentation**:
   * Add docstrings to the new asset function
   * Document configuration parameters and expected behavior
   * Update any relevant documentation referring to the complete Veeva CRM data pipeline

## 6. Success Criteria

* A new `@multi_file_assets` definition is added to `prometheus/dlt/sources/veeva/assets.py`
* The asset correctly sources CSV files from the designated ADLS path (output of AAA-318)
* Data from the CSV files is successfully loaded into corresponding tables in Snowflake under the defined schema and table prefix
* The DLT pipeline runs successfully for all discovered files
* Appropriate logging and error handling are implemented

## 7. Key Dependencies & Assumptions

* **Dependencies**:
  * The `MultiFileResource` from `prometheus/dlt/integrations/multi_file/resource.py` is correctly implemented and operational
  * DLT is properly configured for Snowflake as a destination
  * AAA-318 is completed, and unpacked CSV files are available in the expected ADLS location
  * Authentication to ADLS and Snowflake is properly configured

* **Assumptions**:
  * CSV files from Veeva CRM unpacked in AAA-318 have a consistent structure
  * The ADLS location is accessible with the configured authentication
  * Snowflake destination is configured and accessible
  * File naming and structure from AAA-318 follows the documented pattern
  * The volume of data is manageable for batch loading (no streaming requirements)

## 8. Future Considerations (Post-MVP)

* **Incremental Loading**: Enhance the loading process to support incremental loading based on modification time or other criteria
* **Schema Evolution**: Implement mechanism to handle schema changes in source data
* **Validation & Quality Checks**: Add data validation and quality checks before loading into Snowflake
* **Performance Optimization**: Fine-tune DLT pipeline options for better performance with large files
* **Monitoring & Alerting**: Implement more robust monitoring and alerting for pipeline status
* **Integration with Upstream Assets**: Automate triggering based on successful completion of AAA-318
* **Historical Data Loading**: Develop process for loading historical data if needed

## 9. Documentation Requirements

* **Code Documentation**:
  * Add docstrings to all new functions and classes
  * Document expected parameters and behavior

* **Configuration Documentation**:
  * Document the multi-file asset configuration, including how it maps ADLS files to Snowflake tables
  * Update `.dlt/config.toml` with appropriate comments explaining the Veeva CRM Direct Data configuration

* **Pipeline Documentation**:
  * Update documentation to explain the complete pipeline from Veeva CRM Direct Data API to Snowflake
  * Document integration points between AAA-317, AAA-318, and AAA-319

* **Repository Structure**:
  * Update `docs/repository_structure.md` to reflect any new files or directories